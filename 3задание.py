# -*- coding: utf-8 -*-
"""3задание.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13gaA_8myEywozBlC_AmitMFjUT-sIwkM
"""

from google.colab import drive
drive.mount('gdrive')

input_path = 'gdrive/My Drive/4курс/Инфопоиск/result/'
output_inverted_path = 'gdrive/My Drive/4курс/Инфопоиск/result/inverted_index.txt'

from enum import unique
from itertools import chain
import re
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')
sw_nltk = stopwords.words('english')


def get_tokens_from_file(input_path):

    with open(input_path) as source:

        #get the text from file
        text = source.read()
        words = text.split()

    return words


def get_words_from_file(input):

    # with open(input) as source, open(output, 'w') as destination:
    with open(input) as source:
        #get the text from file
        text = source.read()
        #lower the text
        text = text.lower()
        #split words in text
        words = text.split()
        #get words without number
        words = [re.sub(r'\d', '', word) for word in words]
        words = [re.sub(r'amott/', '', word) for word in words]
        words = [re.sub(r'\b\w{1,2}\b', '', word) for word in words]
        words = [word.replace("'", '') for word in words]
        #reduce stopwords .,!;()[]-:"/\|$@^''&*%?
        words = [word.strip('.,!;()[]-:"/\|$@^''&*%?') for word in words]
        #reduce stopwords
        words = [word for word in words if word not in sw_nltk]
    return words


def get_tokens_from_multiple_files(input_path):
    
    tokens_list = get_tokens_from_file(input_path + 'tokens.txt')

    all_words = []

    #get all words from 142 files
    for i in range(142):
        words = get_words_from_file(input_path + 'выкачка' + str(i + 1) + '.txt')
        all_words.append(words)


    #i - file number, j - position of token in i file
    for k in range(len(tokens_list)):
        for i in range(len(all_words)):
            for j in range(len(all_words[i])):
                if all_words[i][j] == tokens_list[k]:
                    tokens_list[k] +=  ' ' + str(i) + ' ' + str(j)

    print(tokens_list)
    return tokens_list, all_words
  

def write_tokens_to_file(output, unique_words):
    with open(output, 'w') as destination:
        for unique_word in unique_words:
            destination.write(unique_word + '\n')

def binary_search(tokens_list, all_words):
    check_list = []
    for i in range(len(all_words)):
      left = 0

      right = len(all_words[i]) - 1
      target = int(tokens_list[i].split()[1])
      
      while left <= right:
          mid = int((left + right) / 2)
          if mid == target:
              check_list.append(all_words[i][mid])
          elif target < mid:
              right = mid - 1
          else:
              right = mid + 1

res = get_tokens_from_multiple_files(input_path)
tokens = res[0]
all_words = res[1]
write_tokens_to_file(output_inverted_path, tokens)
binary_search(tokens, all_words)